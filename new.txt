important link :
https://www8.cs.umu.se/kurser/TDBA77/VT06/algorithms/BOOK/BOOK5/NODE205.HTM
https://en.wikipedia.org/wiki/Lossless_compression#General_purpose
https://www.quora.com/What-is-the-best-text-compression-algorithm
https://www.quora.com/What-is-the-best-data-compression-algorithm
https://en.wikipedia.org/wiki/DEFLATE#Duplicate_string_elimination
http://www.esrf.eu/computing/Forum/imgCIF/PAPER/huffman.html

General purpose:
Run-length encoding (RLE) – Simple scheme that provides good compression of data containing lots of runs of the same value
Huffman coding – Pairs well with other algorithms, used by Unix's pack utility
Prediction by partial matching (PPM) – Optimized for compressing plain text
bzip2 – Combines Burrows–Wheeler transform with RLE and Huffman coding
Lempel-Ziv compression (LZ77 and LZ78) – Dictionary-based algorithm that forms the basis for many other algorithms
DEFLATE – Combines Lempel-Ziv compression with Huffman coding, used by ZIP, gzip, and PNG images
Lempel–Ziv–Markov chain algorithm (LZMA) – Very high compression ratio, used by 7zip and xz
Lempel–Ziv–Oberhumer (LZO) – Designed for compression/decompression speed at the expense of compression ratios
Lempel–Ziv–Storer–Szymanski (LZSS) – Used by WinRAR in tandem with Huffman coding



The boundary-pushing compressors combine algorithms for insane results. Common algorithms include:

The Burrows-Wheeler Transform and here - shuffle characters (or other bit blocks) with a predictable algorithm to increase repeated blocks which makes the source easier to compress. 
Decompression occurs as normal and the result is un-shuffled with the reverse transform.
 Note: BWT alone doesn't actually compress anything. It just makes the source easier to compress.
Prediction by Partial Matching (PPM) - an evolution of arithmetic coding where the prediction model(context) is created by crunching statistics about the source versus using static probabilities.
 Even though its roots are in arithmetic coding, the result can be represented with Huffman encoding or a dictionary as well as arithmetic coding.
Context Mixing - Arithmetic coding uses a static context for prediction, PPM dynamically chooses a single context, Context Mixing uses many contexts and weighs their results.
PAQ uses context mixing. Here's a high-level overview.
Dynamic Markov Compression - related to PPM but uses bit-level contexts versus byte or longer.
In addition, the Hutter prize contestants may replace common text with small-byte entries from external dictionaries and differentiate upper and lower case text with a special symbol versus using two distinct entries.
That's why they're so good at compressing text (especially ASCII text) and not as valuable for general compression.
Maximum Compression is a pretty cool text and general compression benchmark site.
 Matt Mahoney publishes another benchmark. Mahoney's may be of particular interest because it lists the primary algorithm used per entry.There's always lzip.

All kidding aside:

Where compatibility is a concern, PKZIP (DEFLATE algorithm) still wins.
bzip2 is the best compromise between being enjoying a relatively broad install base and a rather good compression ratio, but requires a separate archiver.
7-Zip (LZMA algorithm) compresses very well and is available for under the LGPL. Few operating systems ship with built-in support, however.
rzip is a variant of bzip2 that in my opinion deserves more attention.
 It could be particularly interesting for huge log files that need long-term archiving. It also requires a separate archiver



